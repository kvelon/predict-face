{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from models import *\n",
    "from data.data_classes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_filepath = \"./plots/live_plot3.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "data_path = \"data/harness/combined.npy\"\n",
    "num_ctx_frames = 5\n",
    "num_tgt_frames = 5\n",
    "split_ratio=[0, 1.0, 0.0]\n",
    "\n",
    "liveness_datamodule = LivenessDataModule(batch_size, \n",
    "                                         num_ctx_frames, num_tgt_frames,\n",
    "                                         data_path,\n",
    "                                         split_ratio=split_ratio)\n",
    "\n",
    "liveness_datamodule.setup()\n",
    "\n",
    "val_dl = liveness_datamodule.val_dataloader()\n",
    "val_ctx_frames, val_tgt_frames = next(iter(val_dl))                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PredRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Experiment 1 (filtered)\n",
    "# input_channels=3\n",
    "# num_hidden=[64, 64, 64]\n",
    "# kernel_size=5\n",
    "# stride=1\n",
    "# learning_rate=1e-3\n",
    "\n",
    "# model = PredRNN(input_channels=input_channels,\n",
    "#                 num_hidden=num_hidden,\n",
    "#                 num_ctx_frames=num_ctx_frames,\n",
    "#                 num_tgt_frames=num_tgt_frames,\n",
    "#                 kernel_size=kernel_size,\n",
    "#                 stride=stride)\n",
    "# model = model.load_from_checkpoint(\"./logs/PredRNN/experiment1/checkpoints/epoch=99-step=1000.ckpt\")\n",
    "\n",
    "# # Experiment 2 (filtered and augmented)\n",
    "# model = PredRNN(input_channels=input_channels,\n",
    "#                 num_hidden=num_hidden,\n",
    "#                 num_ctx_frames=num_ctx_frames,\n",
    "#                 num_tgt_frames=num_tgt_frames,\n",
    "#                 kernel_size=kernel_size,\n",
    "#                 stride=stride)\n",
    "# model = model.load_from_checkpoint(\"./logs/PredRNN/experiment2/checkpoints/epoch=99-step=4000.ckpt\")\n",
    "\n",
    "# # Experiment 3 (unfiltered)\n",
    "# model = PredRNN(input_channels=input_channels,\n",
    "#                 num_hidden=num_hidden,\n",
    "#                 num_ctx_frames=num_ctx_frames,\n",
    "#                 num_tgt_frames=num_tgt_frames,\n",
    "#                 kernel_size=kernel_size,\n",
    "#                 stride=stride)\n",
    "# model = model.load_from_checkpoint(\"./logs/PredRNN/experiment3/checkpoints/epoch=99-step=4000.ckpt\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted Frames Set 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Experiment 3 (unfiltered)\n",
    "# input_channels=3\n",
    "# num_hidden=[64, 64, 64]\n",
    "# kernel_size=5\n",
    "# stride=1\n",
    "# learning_rate=1e-3\n",
    "\n",
    "# model = PredRNN(input_channels=input_channels,\n",
    "#                 num_hidden=num_hidden,\n",
    "#                 num_ctx_frames=num_ctx_frames,\n",
    "#                 num_tgt_frames=num_tgt_frames,\n",
    "#                 kernel_size=kernel_size,\n",
    "#                 stride=stride)\n",
    "# model = model.load_from_checkpoint(\"./logs/PredRNN/experiment3/checkpoints/epoch=99-step=4000.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# height = 280\n",
    "# width = 160\n",
    "\n",
    "# model1_pred_frames = torch.zeros(batch_size, 3, 10, height, width)\n",
    "# model1_pred_frames[:, :, :5] = val_ctx_frames\n",
    "\n",
    "# model.eval()\n",
    "# for i in range(5, 10):\n",
    "#     predrnn_input = torch.zeros(batch_size, 3, 6, height, width)\n",
    "#     predrnn_input[:, :, :5] = model1_pred_frames[:, :, i-5:i]\n",
    "#     predrnn_input[:, :, 5] =  model1_pred_frames[:, :, i-1]\n",
    "#     model1_pred_frames[:, :, i] = model(predrnn_input)[:, :, -1]  # Keep last frame only\n",
    "\n",
    "# torch.save(model1_pred_frames, \"./plots/5-5-predrnn-uf.pt\")\n",
    "# model1_pred_frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_pred_frames = torch.load(\"./plots/5-5-predrnn-uf.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted Frames Set 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Experiment 1 (filtered)\n",
    "# input_channels=3\n",
    "# num_hidden=[64, 64, 64]\n",
    "# kernel_size=5\n",
    "# stride=1\n",
    "# learning_rate=1e-3\n",
    "\n",
    "# model = PredRNN(input_channels=input_channels,\n",
    "#                 num_hidden=num_hidden,\n",
    "#                 num_ctx_frames=num_ctx_frames,\n",
    "#                 num_tgt_frames=num_tgt_frames,\n",
    "#                 kernel_size=kernel_size,\n",
    "#                 stride=stride)\n",
    "# model = model.load_from_checkpoint(\"./logs/PredRNN/experiment1/checkpoints/epoch=99-step=1000.ckpt\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# height = 280\n",
    "# width = 160\n",
    "\n",
    "# model2_pred_frames = torch.zeros(batch_size, 3, 10, height, width)\n",
    "# model2_pred_frames[:, :, :5] = val_ctx_frames\n",
    "\n",
    "# model.eval()\n",
    "# for i in range(5, 10):\n",
    "#     predrnn_input = torch.zeros(batch_size, 3, 6, height, width)\n",
    "#     predrnn_input[:, :, :5] = model2_pred_frames[:, :, i-5:i]\n",
    "#     predrnn_input[:, :, 5] =  model2_pred_frames[:, :, i-1]\n",
    "#     model2_pred_frames[:, :, i] = model(predrnn_input)[:, :, -1]  # Keep last frame only\n",
    "\n",
    "# torch.save(model2_pred_frames, \"./plots/5-5-predrnn-ft.pt\")\n",
    "# model2_pred_frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_pred_frames = torch.load(\"./plots/5-5-predrnn-ft.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted Frames Set 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Experiment 2 (filtered and augmented)\n",
    "# input_channels=3\n",
    "# num_hidden=[64, 64, 64]\n",
    "# kernel_size=5\n",
    "# stride=1\n",
    "# learning_rate=1e-3\n",
    "\n",
    "# model = PredRNN(input_channels=input_channels,\n",
    "#                 num_hidden=num_hidden,\n",
    "#                 num_ctx_frames=num_ctx_frames,\n",
    "#                 num_tgt_frames=num_tgt_frames,\n",
    "#                 kernel_size=kernel_size,\n",
    "#                 stride=stride)\n",
    "# model = model.load_from_checkpoint(\"./logs/PredRNN/experiment2/checkpoints/epoch=99-step=4000.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# height = 280\n",
    "# width = 160\n",
    "\n",
    "# model3_pred_frames = torch.zeros(batch_size, 3, 10, height, width)\n",
    "# model3_pred_frames[:, :, :5] = val_ctx_frames\n",
    "\n",
    "# model.eval()\n",
    "# for i in range(5, 10):\n",
    "#     predrnn_input = torch.zeros(batch_size, 3, 6, height, width)\n",
    "#     predrnn_input[:, :, :5] = model3_pred_frames[:, :, i-5:i]\n",
    "#     predrnn_input[:, :, 5] =  model3_pred_frames[:, :, i-1]\n",
    "#     model3_pred_frames[:, :, i] = model(predrnn_input)[:, :, -1]  # Keep last frame only\n",
    "\n",
    "# torch.save(model3_pred_frames, \"./plots/5-5-predrnn-fa.pt\")\n",
    "# model3_pred_frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_pred_frames = torch.load(\"./plots/5-5-predrnn-fa.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_thesis_plot(frame_sets, names, plot_width, plot_height):\n",
    "\n",
    "    def show_frames(frames, ax, row_label=None):\n",
    "        start_id = 1 if row_label == \"Context\" else 6\n",
    "        for i, frame in enumerate(frames):\n",
    "            ax[i].imshow(frame)\n",
    "            ax[i].set_xticks([])\n",
    "            ax[i].set_yticks([])\n",
    "            ax[i].set_xlabel(f\"t={start_id+i}\")\n",
    "            ax[i].xaxis.set_label_coords(.52, 1.15)\n",
    "\n",
    "        if row_label is not None:\n",
    "            ax[0].set_ylabel(row_label, wrap=True)\n",
    "\n",
    "    fig, ax = plt.subplots(len(frame_sets), 5,\n",
    "                               figsize = (plot_width, plot_height))\n",
    "\n",
    "    for i, frames in enumerate(frame_sets):\n",
    "        frames = frames.squeeze().permute(1, 2, 3, 0).cpu().detach().numpy()\n",
    "        show_frames(frames, ax[i], names[i])\n",
    "    fig.set_facecolor(\"white\")\n",
    "    plt.savefig(save_filepath)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "plot_width = 6\n",
    "plot_height = 6\n",
    "frame_sets = [model1_pred_frames[index, :, 5:],\n",
    "              model2_pred_frames[index, :, 5:],\n",
    "              model3_pred_frames[index, :, 5:]]\n",
    "\n",
    "names = [\"PredRNN-\\nUnfiltered\", \n",
    "         \"PredRNN-\\nFiltered\", \"PredRNN-\\nFiltered and Augmented\"]               \n",
    "make_thesis_plot(frame_sets, names, plot_width, plot_height)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da4c3a4107fc661dfc1ddc51b98664f856b9baf685ab1745d9fa2472938977d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
